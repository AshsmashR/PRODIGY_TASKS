# -*- coding: utf-8 -*-
"""TEXT GENERATION MARKOV CHAINS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rD-9D9-B1aDK3tCdwUGTsQaDJsDpsrw9
"""

#LOAD MY DATASET

import pandas as pd
file_path = r"/content/Friends.csv"
data = pd.read_csv(file_path)
print(data.head())

#CHECK MY DATASET
print(data.info())
print(data.isnull().sum())

#CLEAN DATASET
import re

def clean_text(text):
    """
    Cleans the input text by:
    - Removing non-alphanumeric characters
    - Converting to lowercase
    - Removing extra whitespaces
    """
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
    text = text.lower()
    text = re.sub(r"\s+", " ", text).strip()
    return text

data['Cleaned_Input_Text'] = data['Input Text'].apply(clean_text)
data['Cleaned_Target_Text'] = data['Target Text'].apply(clean_text)
print("\nCleaned Data:\n", data[['Cleaned_Input_Text', 'Cleaned_Target_Text']])

#DATASET I GOT FROM KAGGLE HAS TEXT DATA AND LABEL FOR THE SENTENCES

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer #type: ignore
from tensorflow.keras.preprocessing.sequence import pad_sequences #type:ignore


#IMPORTANT STEP FOR TEXT GENERATION
#tokenised the input and target coloumns

input_texts = data['Cleaned_Input_Text'].tolist()
target_texts = data['Cleaned_Target_Text'].tolist()
input_tokenizer = Tokenizer()
target_tokenizer = Tokenizer()

#converted them into sequences
input_tokenizer.fit_on_texts(input_texts)
target_tokenizer.fit_on_texts(target_texts)
input_sequences = input_tokenizer.texts_to_sequences(input_texts)
target_sequences = target_tokenizer.texts_to_sequences(target_texts)

#padding to ensure even length of sentences
input_sequences_padded = pad_sequences(input_sequences, padding='post')
target_sequences_padded = pad_sequences(target_sequences, padding='post')

print(input_sequences_padded)
print(target_sequences_padded)

#lets convert those tokens to tensors for ease of machine understanding as it is like a matrice

input_tensor = tf.convert_to_tensor(input_sequences_padded, dtype=tf.int32)
target_tensor = tf.convert_to_tensor(target_sequences_padded, dtype=tf.int32)
print(f"Input Tensor Shape: {input_tensor.shape}")
print(f"Target Tensor Shape: {target_tensor.shape}")

# Input tokenizer details
input_vocab_size = len(input_tokenizer.word_index) + 1  # +1 for padding token
print(f"Input Vocabulary Size: {input_vocab_size}")
print(f"Input Tokenizer Word Index:\n{input_tokenizer.word_index}")
target_vocab_size = len(target_tokenizer.word_index) + 1  # +1 for padding token
print(f"Target Vocabulary Size: {target_vocab_size}")
print(f"Target Tokenizer Word Index:\n{target_tokenizer.word_index}")

import random
from collections import defaultdict

#a markov chain model without bigrams
all_text = input_texts + target_texts
tokenized_sentences = [sentence.split() for sentence in all_text]
markov_chain = defaultdict(list)

for tokens in tokenized_sentences:
    for i in range(len(tokens) - 1):
        current_word = tokens[i]
        next_word = tokens[i + 1]
        markov_chain[current_word].append(next_word)

print("Markov Chain Model:")
for key, values in markov_chain.items():
    print(f"{key}: {values}")

def generate_text(chain, start_word, max_length=20):
    """
    Generates text using a Markov Chain.

    Args:
    - chain: The Markov Chain dictionary.
    - start_word: The word to start the generation with.
    - max_length: Maximum number of words in the generated text.

    Returns:
    - A generated string.
    """
    current_word = start_word
    generated_text = [current_word]

    for _ in range(max_length - 1):
        if current_word not in chain or len(chain[current_word]) == 0:
            break
        next_word = random.choice(chain[current_word])
        generated_text.append(next_word)
        current_word = next_word

    return " ".join(generated_text)
start_word = "friends"
generated_text = generate_text(markov_chain, start_word, max_length=20)
print("\nGenerated Text:")
print(generated_text)
#i used only 1 start word now

def text_no_repeat(chain, start_word, max_length=20):

    current_word = start_word
    generated_text = [current_word]
    for _ in range(max_length - 1):
        if current_word not in chain or len(chain[current_word]) == 0:
            break
        next_word_options = [word for word in chain[current_word] if word != current_word]
        if not next_word_options:
            break
        next_word = random.choice(next_word_options)
        generated_text.append(next_word)
        current_word = next_word

    return " ".join(generated_text)

start_word = "friends"
non_repetitive_text = text_no_repeat(markov_chain, start_word, max_length=20)
print("\nNon-Repetitive Generated Text:")
print(non_repetitive_text)

markov_chain_bigrams = defaultdict(list)

for tokens in tokenized_sentences:
    for i in range(len(tokens) - 2):
        current_pair = (tokens[i], tokens[i + 1])  # Create a pair of consecutive words
        next_word = tokens[i + 2]
        markov_chain_bigrams[current_pair].append(next_word)

print("\nMarkov Chain with Bigrams:")
for key, values in markov_chain_bigrams.items():
    print(f"{key}: {values}")
#now lets undertand bigrams

def generate_text_bigrams(chain, start_pair, max_length=10):
    """
    Generates text using a bigram-based Markov Chain.

    Args:
    - chain: The Markov Chain dictionary with bigrams.
    - start_pair: A tuple containing the starting pair of words.
    - max_length: Maximum number of words in the generated text.

    Returns:
    - A generated string.
    """
    current_pair = start_pair
    generated_text = list(current_pair)

    for _ in range(max_length - 2):
        if current_pair not in chain or len(chain[current_pair]) == 0:
            break  # Stop if no next word is available
        next_word = random.choice(chain[current_pair])
        generated_text.append(next_word)
        current_pair = (current_pair[1], next_word)  # Update the current pair

    return " ".join(generated_text)
start_pair = ("last", "a")
bigram_generated_text = generate_text_bigrams(markov_chain_bigrams, start_pair, max_length=10)
print(bigram_generated_text)

def generate_text_with_fallback(chain, start_word, max_length=100):
    """
    Generates text using a Markov Chain with a fallback for unknown start words.
    """
    if start_word not in chain:
        start_word = random.choice(list(chain.keys()))
        print(f"Fallback to random start word: {start_word}")
    current_word = start_word
    generated_text = [current_word]

    for _ in range(max_length - 1):
        if current_word not in chain or len(chain[current_word]) == 0:
            break
        next_word = random.choice(chain[current_word])
        generated_text.append(next_word)
        current_word = next_word

    return " ".join(generated_text)

fallback_generated_text = generate_text_with_fallback(markov_chain, "Friends Are", max_length=100)
print("\nGenerated Text with Fallback:")
print(fallback_generated_text)
#this is for random words that i may type !!